{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a7b738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/uvadm/zhenyu/miniconda3/envs/mechanistic/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtyping import TensorType as TT\n",
    "from typing import List, Union, Optional, Callable\n",
    "from functools import partial\n",
    "import copy\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20494482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysvelte\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aad4f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neel_plotly import line, imshow, scatter\n",
    "import transformer_lens.patching as patching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7abb358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "model.set_use_attn_result(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5952438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0736c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean string 0 <|endoftext|>When John and Mary went to the shops, John gave the bag to\n",
      "Corrupted string 0 <|endoftext|>When John and Mary went to the shops, Mary gave the bag to\n",
      "Answer token indices tensor([[ 5335,  1757],\n",
      "        [ 1757,  5335],\n",
      "        [ 4186,  3700],\n",
      "        [ 3700,  4186],\n",
      "        [ 6035, 15686],\n",
      "        [15686,  6035],\n",
      "        [ 5780, 14235],\n",
      "        [14235,  5780]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "prompts = ['When John and Mary went to the shops, John gave the bag to', 'When John and Mary went to the shops, Mary gave the bag to', 'When Tom and James went to the park, James gave the ball to', 'When Tom and James went to the park, Tom gave the ball to', 'When Dan and Sid went to the shops, Sid gave an apple to', 'When Dan and Sid went to the shops, Dan gave an apple to', 'After Martin and Amy went to the park, Amy gave a drink to', 'After Martin and Amy went to the park, Martin gave a drink to']\n",
    "answers = [(' Mary', ' John'), (' John', ' Mary'), (' Tom', ' James'), (' James', ' Tom'), (' Dan', ' Sid'), (' Sid', ' Dan'), (' Martin', ' Amy'), (' Amy', ' Martin')]\n",
    "\n",
    "clean_tokens = model.to_tokens(prompts)\n",
    "# Swap each adjacent pair, with a hacky list comprehension\n",
    "corrupted_tokens = clean_tokens[\n",
    "    [(i+1 if i%2==0 else i-1) for i in range(len(clean_tokens)) ]\n",
    "    ]\n",
    "print(\"Clean string 0\", model.to_string(clean_tokens[0]))\n",
    "print(\"Corrupted string 0\", model.to_string(corrupted_tokens[0]))\n",
    "\n",
    "answer_token_indices = torch.tensor([[model.to_single_token(answers[i][j]) for j in range(2)] for i in range(len(answers))], device=model.cfg.device)\n",
    "print(\"Answer token indices\", answer_token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693f62d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit diff: 3.5519\n",
      "Corrupted logit diff: -3.5519\n",
      "Clean Baseline is 1: 1.0000\n",
      "Corrupted Baseline is 0: 0.0000\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def get_logit_diff(logits, answer_token_indices=answer_token_indices):\n",
    "    if len(logits.shape)==3:\n",
    "        # Get final logits only\n",
    "        logits = logits[:, -1, :]\n",
    "    correct_logits = logits.gather(1, answer_token_indices[:, 0].unsqueeze(1))\n",
    "    incorrect_logits = logits.gather(1, answer_token_indices[:, 1].unsqueeze(1))\n",
    "    return (correct_logits - incorrect_logits).mean()\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "\n",
    "clean_logit_diff = get_logit_diff(clean_logits, answer_token_indices).item()\n",
    "print(f\"Clean logit diff: {clean_logit_diff:.4f}\")\n",
    "\n",
    "corrupted_logit_diff = get_logit_diff(corrupted_logits, answer_token_indices).item()\n",
    "print(f\"Corrupted logit diff: {corrupted_logit_diff:.4f}\")\n",
    "\n",
    "\n",
    "CLEAN_BASELINE = clean_logit_diff\n",
    "CORRUPTED_BASELINE = corrupted_logit_diff\n",
    "def ioi_metric(logits, answer_token_indices=answer_token_indices):\n",
    "    return (get_logit_diff(logits, answer_token_indices) - CORRUPTED_BASELINE) / (CLEAN_BASELINE  - CORRUPTED_BASELINE)\n",
    "\n",
    "print(f\"Clean Baseline is 1: {ioi_metric(clean_logits).item():.4f}\")\n",
    "print(f\"Corrupted Baseline is 0: {ioi_metric(corrupted_logits).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f113424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa6b9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mechanistic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
